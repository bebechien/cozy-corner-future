+++
title = "初めてのRAGシステム構築"
date = "2026-02-12"
cover = "images/building-your-first-rag-system.png"
tags = ["EmbeddingGemma", "Gemma", "AI"]
description = ""
showFullContent = false
readingTime = false
hideComments = false
emoji = "📚"
+++

# 💎 鉱石の採掘からインサイトの発掘へ

*[Core Keeper（コアキーパー）](https://en.wikipedia.org/wiki/Core_Keeper)* の地下バイオームを探索しているときでも、スモールビジネスの複雑なスプレッドシートと格闘しているときでも、「情報過多」は私たちのラスボスです。

**RAG（検索拡張生成、Retrieval-Augmented Generation）** システムは、AIに専用のガイドブックを渡すようなものです。一般的な学習データに頼るのではなく、ゲームのWikiや研究ノート、法的な契約書といった「あなた独自のデータ」を参照することで、ピンポイントで正確な答えを出してくれます。

ここでは、**[Gemma 3 4B](https://huggingface.co/google/gemma-3-4b-it)** と **[EmbeddingGemma](https://huggingface.co/google/embeddinggemma-300m)** を使って、プライベートなローカルナレッジベースを構築する方法をご紹介します。

# 🛠️ 「クラフト」ステーション（技術スタック）

これを作るにあたって、「ローカルファースト」のアプローチをとります。つまり、データがあなたのPCから外に出ることは決してないということです。秘密基地の座標（あるいは大切なクライアント情報）を安全に保つには完璧ですね。

* **頭脳（LLM）:** `gemma3:4b` - Googleのコンパクトで非常に効率的なモデルです。
* **司書（エンベッダー）:** `embeddinggemma` - 検索できるようにデータを「インデックス化」する特化型モデルです。
* **サーバー:** **[Ollama](https://ollama.com/)** - 自分のPCでこれらのモデルを動かすためのエンジンです。
* **インターフェース:** **[AnythingLLM](https://anythingllm.com/)** - チャット画面のような見た目ですが、文書保存などの面倒な作業をすべてこなしてくれる使いやすいアプリです。

メモ：2026年のローカルAIの最高の魅力の一つは、ツールが「プラグアンドプレイ」であることです。技術的なスキルに合わせて、サーバーとUIを自由に組み合わせて使うことができます。たとえば、Ollamaの代わりに[LM Studio](https://lmstudio.ai/)を使ったり、AnythingLLMの代わりに[Open WebUI](https://openwebui.com/)を使ったりすることも可能です。色んなツールを試してみてください！

# 📖 ステップ1：素材を集める

まずは、「信頼できる情報源（Source of Truth）」を特定しましょう。

* **ゲーマーなら:** ボスの攻略法やクラフトのレシピをまとめるために、*[Core Keeper Wiki](https://corekeeper.atma.gg/en/Core_Keeper_Wiki)* を使ってみましょう。
* **プロフェッショナルなら:** PDFのフォルダやプロジェクトのログ、あるいは専門的なウェブサイトなどが該当します。

# ⚙️ ステップ2：ワークショップの準備（Ollama）

*（ちょっとしたヒント：このような4Bモデルをスムーズに動かすには、約8GBのVRAMがあると良いですよ！）*

Ollamaをダウンロードして、ターミナルで以下の2つのコマンドを実行し、「モデル」をダウンロードしてください。

```shell
# Download the language model
ollama pull gemma3:4b

# Download the embedding model
ollama pull embeddinggemma

```

# 🖥️ ステップ3：インターフェースの設定（AnythingLLM）

**AnythingLLM**を開き、以下の手順に従ってモデルを連携させます。

1. **LLM設定:** プロバイダーを**Ollama**に設定し、`gemma3:4b`を選択します。このモデルは、検索されたコンテキスト（文脈）を読み取り、最終的な回答を組み立てる「スピーカー（回答者）」として機能します。
2. **エンベッダー設定:** **Ollama**を選び、`embeddinggemma`を選択してください。このモデルは、「検索エンジン」として機能する専用の高性能エンベディングモデルです。
3. **アップロード:** 「ワークスペース（Workspace）」を作成し、ファイル（*Core Keeper*のWikiページや仕事の書類など）をドロップします。そして **「Save and Embed（保存して埋め込む）」** をクリックします。

# ⚔️ ステップ4：ナレッジベースを活用する

さあ、あなたのデータとチャットしてみましょう。

* **ゲームの質問:** *"忌まわしき大質量（Abominouse Mass）はどうやって倒せばいい？"*
* **仕事の質問:** *"第3四半期のマーケティング契約の主要な条項を要約して。"*

Gemma 3 4Bは単に「推測」するだけでなく、あなたのファイルから特定のテキストを見つけ出して解説してくれます。

独自のナレッジベースを持つ前と後で、AIに質問した時の違いを見てみましょう。

* Before: 文脈がないため、AIが一般的で曖昧、あるいは間違った答えを返している様子。

![before](images/building-your-first-rag-system-before.png)

* After: AIがアップロードした文書を引用元として提示し、正確で的確な答えを返している様子。

![after](images/building-your-first-rag-system-after.png)

# 👨🏻‍💻 自分のデータに応用してみよう

これを自分で構築することで、あなたは単なるAIユーザーではなく「設計者（アーキテクト）」になります。ゲームのプレイを最適化する場合でも、仕事のワークフローを改善する場合でも、あなたが知っていることを正確に把握している100%プライベートなオフラインアシスタントを手に入れたことになります。

ここまでは*Core Keeper*を例に挙げてきましたが、この「ビルド」は専門的な現場仕事における救世主にもなります。

* **フィールド研究者:** **インターネットが全く使えない**大自然の僻地にいると想像してみてください。出発前に、植物図鑑、過去の探検記録、地質図など、ライブラリ全体をAIに学習させておくことができます。
* **作家:** 大切な知的財産（IP）をクラウドにアップロードすることなく、下書きの章を読み込ませて世界観の矛盾をチェックできます。
* **ホームシェフ:** ごちゃごちゃになったレシピのスクリーンショットフォルダを、検索可能な「デジタル料理本」に変えることができます。

> **最大のメリット:** **Gemma 3 4B**と**EmbeddingGemma**をローカルで使用するため、システムは100%**オフライン**で動作します。データがPCの外に出ることは一切ないため、衛星回線なしで即座に答えが必要な現場の研究者にとって、完璧なパートナーとなります。

