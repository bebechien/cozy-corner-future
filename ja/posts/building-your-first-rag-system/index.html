<!doctype html><html lang=ja><head><title>初めてのRAGシステム構築 :: すみっコの未来</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="💎 鉱石の採掘からインサイトの発掘へ Core Keeper（コアキーパー） の地下バイオームを探索しているときでも、スモールビジネスの複雑なスプレッドシートと格闘しているときでも、「情報過多」は私たちのラスボスです。
RAG（検索拡張生成、Retrieval-Augmented Generation） システムは、AIに専用のガイドブックを渡すようなものです。一般的な学習データに頼るのではなく、ゲームのWikiや研究ノート、法的な契約書といった「あなた独自のデータ」を参照することで、ピンポイントで正確な答えを出してくれます。
ここでは、Gemma 3 4B と EmbeddingGemma を使って、プライベートなローカルナレッジベースを構築する方法をご紹介します。
🛠️ 「クラフト」ステーション（技術スタック） これを作るにあたって、「ローカルファースト」のアプローチをとります。つまり、データがあなたのPCから外に出ることは決してないということです。秘密基地の座標（あるいは大切なクライアント情報）を安全に保つには完璧ですね。
頭脳（LLM）: gemma3:4b - Googleのコンパクトで非常に効率的なモデルです。 司書（エンベッダー）: embeddinggemma - 検索できるようにデータを「インデックス化」する特化型モデルです。 サーバー: Ollama - 自分のPCでこれらのモデルを動かすためのエンジンです。 インターフェース: AnythingLLM - チャット画面のような見た目ですが、文書保存などの面倒な作業をすべてこなしてくれる使いやすいアプリです。 メモ：2026年のローカルAIの最高の魅力の一つは、ツールが「プラグアンドプレイ」であることです。技術的なスキルに合わせて、サーバーとUIを自由に組み合わせて使うことができます。たとえば、Ollamaの代わりにLM Studioを使ったり、AnythingLLMの代わりにOpen WebUIを使ったりすることも可能です。色んなツールを試してみてください！
📖 ステップ1：素材を集める まずは、「信頼できる情報源（Source of Truth）」を特定しましょう。
ゲーマーなら: ボスの攻略法やクラフトのレシピをまとめるために、Core Keeper Wiki を使ってみましょう。 プロフェッショナルなら: PDFのフォルダやプロジェクトのログ、あるいは専門的なウェブサイトなどが該当します。 ⚙️ ステップ2：ワークショップの準備（Ollama） （ちょっとしたヒント：このような4Bモデルをスムーズに動かすには、約8GBのVRAMがあると良いですよ！）
Ollamaをダウンロードして、ターミナルで以下の2つのコマンドを実行し、「モデル」をダウンロードしてください。
# Download the language model ollama pull gemma3:4b # Download the embedding model ollama pull embeddinggemma 🖥️ ステップ3：インターフェースの設定（AnythingLLM） AnythingLLMを開き、以下の手順に従ってモデルを連携させます。
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://bebechien.github.io/cozy-corner-future/ja/posts/building-your-first-rag-system/><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel=stylesheet href=https://bebechien.github.io/cozy-corner-future/style.css><link rel="shortcut icon" href=https://bebechien.github.io/cozy-corner-future/favicon.png><link rel=apple-touch-icon href=https://bebechien.github.io/cozy-corner-future/apple-touch-icon.png><meta name=twitter:card content="summary"><meta name=twitter:site content><meta name=twitter:creator content><meta property="og:locale" content="ja"><meta property="og:type" content="article"><meta property="og:title" content="初めてのRAGシステム構築"><meta property="og:description" content="💎 鉱石の採掘からインサイトの発掘へ Core Keeper（コアキーパー） の地下バイオームを探索しているときでも、スモールビジネスの複雑なスプレッドシートと格闘しているときでも、「情報過多」は私たちのラスボスです。
RAG（検索拡張生成、Retrieval-Augmented Generation） システムは、AIに専用のガイドブックを渡すようなものです。一般的な学習データに頼るのではなく、ゲームのWikiや研究ノート、法的な契約書といった「あなた独自のデータ」を参照することで、ピンポイントで正確な答えを出してくれます。
ここでは、Gemma 3 4B と EmbeddingGemma を使って、プライベートなローカルナレッジベースを構築する方法をご紹介します。
🛠️ 「クラフト」ステーション（技術スタック） これを作るにあたって、「ローカルファースト」のアプローチをとります。つまり、データがあなたのPCから外に出ることは決してないということです。秘密基地の座標（あるいは大切なクライアント情報）を安全に保つには完璧ですね。
頭脳（LLM）: gemma3:4b - Googleのコンパクトで非常に効率的なモデルです。 司書（エンベッダー）: embeddinggemma - 検索できるようにデータを「インデックス化」する特化型モデルです。 サーバー: Ollama - 自分のPCでこれらのモデルを動かすためのエンジンです。 インターフェース: AnythingLLM - チャット画面のような見た目ですが、文書保存などの面倒な作業をすべてこなしてくれる使いやすいアプリです。 メモ：2026年のローカルAIの最高の魅力の一つは、ツールが「プラグアンドプレイ」であることです。技術的なスキルに合わせて、サーバーとUIを自由に組み合わせて使うことができます。たとえば、Ollamaの代わりにLM Studioを使ったり、AnythingLLMの代わりにOpen WebUIを使ったりすることも可能です。色んなツールを試してみてください！
📖 ステップ1：素材を集める まずは、「信頼できる情報源（Source of Truth）」を特定しましょう。
ゲーマーなら: ボスの攻略法やクラフトのレシピをまとめるために、Core Keeper Wiki を使ってみましょう。 プロフェッショナルなら: PDFのフォルダやプロジェクトのログ、あるいは専門的なウェブサイトなどが該当します。 ⚙️ ステップ2：ワークショップの準備（Ollama） （ちょっとしたヒント：このような4Bモデルをスムーズに動かすには、約8GBのVRAMがあると良いですよ！）
Ollamaをダウンロードして、ターミナルで以下の2つのコマンドを実行し、「モデル」をダウンロードしてください。
# Download the language model ollama pull gemma3:4b # Download the embedding model ollama pull embeddinggemma 🖥️ ステップ3：インターフェースの設定（AnythingLLM） AnythingLLMを開き、以下の手順に従ってモデルを連携させます。
"><meta property="og:url" content="https://bebechien.github.io/cozy-corner-future/ja/posts/building-your-first-rag-system/"><meta property="og:site_name" content="すみっコの未来"><meta property="og:image" content="https://bebechien.github.io/cozy-corner-future/images/building-your-first-rag-system.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-02-12 00:00:00 +0000 UTC"></head><body><div class=container><header class=header><div class=header__inner><div class=header__logo><a href=/cozy-corner-future/ja/><div class=logo>すみっコの未来</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/cozy-corner-future/ja/about>About</a></li><li><a href=/cozy-corner-future/ja/posts>Posts</a></li><hr><li><a href=https://bebechien.github.io/cozy-corner-future/>English</a></li><li><a href=https://bebechien.github.io/cozy-corner-future/ko/>한국어</a></li><li><a href=https://bebechien.github.io/cozy-corner-future/ja/>日本語</a></li></ul></li></ul><ul class="menu menu--desktop menu--language-selector"><li class=menu__trigger>日本語&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=https://bebechien.github.io/cozy-corner-future/>English</a></li><li><a href=https://bebechien.github.io/cozy-corner-future/ko/>한국어</a></li><li><a href=https://bebechien.github.io/cozy-corner-future/ja/>日本語</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/cozy-corner-future/ja/about>About</a></li><li><a href=/cozy-corner-future/ja/posts>Posts</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://bebechien.github.io/cozy-corner-future/ja/posts/building-your-first-rag-system/>初めてのRAGシステム構築</a></h1><div class=post-meta><time class=post-date>2026-02-12</time></div><span class=post-tags>#<a href=https://bebechien.github.io/cozy-corner-future/ja/tags/embeddinggemma/>EmbeddingGemma</a>&nbsp;
#<a href=https://bebechien.github.io/cozy-corner-future/ja/tags/gemma/>Gemma</a>&nbsp;
#<a href=https://bebechien.github.io/cozy-corner-future/ja/tags/ai/>AI</a>&nbsp;
</span><img src=https://bebechien.github.io/cozy-corner-future/images/building-your-first-rag-system.png class=post-cover alt=初めてのRAGシステム構築 title="Cover Image"><div class=post-content><div><h1 id=-鉱石の採掘からインサイトの発掘へ>💎 鉱石の採掘からインサイトの発掘へ<a href=#-鉱石の採掘からインサイトの発掘へ class=hanchor arialabel=Anchor>#</a></h1><p><em><a href=https://en.wikipedia.org/wiki/Core_Keeper>Core Keeper（コアキーパー）</a></em> の地下バイオームを探索しているときでも、スモールビジネスの複雑なスプレッドシートと格闘しているときでも、「情報過多」は私たちのラスボスです。</p><p><strong>RAG（検索拡張生成、Retrieval-Augmented Generation）</strong> システムは、AIに専用のガイドブックを渡すようなものです。一般的な学習データに頼るのではなく、ゲームのWikiや研究ノート、法的な契約書といった「あなた独自のデータ」を参照することで、ピンポイントで正確な答えを出してくれます。</p><p>ここでは、<strong><a href=https://huggingface.co/google/gemma-3-4b-it>Gemma 3 4B</a></strong> と <strong><a href=https://huggingface.co/google/embeddinggemma-300m>EmbeddingGemma</a></strong> を使って、プライベートなローカルナレッジベースを構築する方法をご紹介します。</p><h1 id=-クラフトステーション技術スタック>🛠️ 「クラフト」ステーション（技術スタック）<a href=#-クラフトステーション技術スタック class=hanchor arialabel=Anchor>#</a></h1><p>これを作るにあたって、「ローカルファースト」のアプローチをとります。つまり、データがあなたのPCから外に出ることは決してないということです。秘密基地の座標（あるいは大切なクライアント情報）を安全に保つには完璧ですね。</p><ul><li><strong>頭脳（LLM）:</strong> <code>gemma3:4b</code> - Googleのコンパクトで非常に効率的なモデルです。</li><li><strong>司書（エンベッダー）:</strong> <code>embeddinggemma</code> - 検索できるようにデータを「インデックス化」する特化型モデルです。</li><li><strong>サーバー:</strong> <strong><a href=https://ollama.com/>Ollama</a></strong> - 自分のPCでこれらのモデルを動かすためのエンジンです。</li><li><strong>インターフェース:</strong> <strong><a href=https://anythingllm.com/>AnythingLLM</a></strong> - チャット画面のような見た目ですが、文書保存などの面倒な作業をすべてこなしてくれる使いやすいアプリです。</li></ul><p>メモ：2026年のローカルAIの最高の魅力の一つは、ツールが「プラグアンドプレイ」であることです。技術的なスキルに合わせて、サーバーとUIを自由に組み合わせて使うことができます。たとえば、Ollamaの代わりに<a href=https://lmstudio.ai/>LM Studio</a>を使ったり、AnythingLLMの代わりに<a href=https://openwebui.com/>Open WebUI</a>を使ったりすることも可能です。色んなツールを試してみてください！</p><h1 id=-ステップ1素材を集める>📖 ステップ1：素材を集める<a href=#-ステップ1素材を集める class=hanchor arialabel=Anchor>#</a></h1><p>まずは、「信頼できる情報源（Source of Truth）」を特定しましょう。</p><ul><li><strong>ゲーマーなら:</strong> ボスの攻略法やクラフトのレシピをまとめるために、<em><a href=https://corekeeper.atma.gg/en/Core_Keeper_Wiki>Core Keeper Wiki</a></em> を使ってみましょう。</li><li><strong>プロフェッショナルなら:</strong> PDFのフォルダやプロジェクトのログ、あるいは専門的なウェブサイトなどが該当します。</li></ul><h1 id=-ステップ2ワークショップの準備ollama>⚙️ ステップ2：ワークショップの準備（Ollama）<a href=#-ステップ2ワークショップの準備ollama class=hanchor arialabel=Anchor>#</a></h1><p><em>（ちょっとしたヒント：このような4Bモデルをスムーズに動かすには、約8GBのVRAMがあると良いですよ！）</em></p><p>Ollamaをダウンロードして、ターミナルで以下の2つのコマンドを実行し、「モデル」をダウンロードしてください。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># Download the language model</span>
</span></span><span class=line><span class=cl>ollama pull gemma3:4b
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Download the embedding model</span>
</span></span><span class=line><span class=cl>ollama pull embeddinggemma
</span></span></code></pre></div><h1 id=-ステップ3インターフェースの設定anythingllm>🖥️ ステップ3：インターフェースの設定（AnythingLLM）<a href=#-ステップ3インターフェースの設定anythingllm class=hanchor arialabel=Anchor>#</a></h1><p><strong>AnythingLLM</strong>を開き、以下の手順に従ってモデルを連携させます。</p><ol><li><strong>LLM設定:</strong> プロバイダーを<strong>Ollama</strong>に設定し、<code>gemma3:4b</code>を選択します。このモデルは、検索されたコンテキスト（文脈）を読み取り、最終的な回答を組み立てる「スピーカー（回答者）」として機能します。</li><li><strong>エンベッダー設定:</strong> <strong>Ollama</strong>を選び、<code>embeddinggemma</code>を選択してください。このモデルは、「検索エンジン」として機能する専用の高性能エンベディングモデルです。</li><li><strong>アップロード:</strong> 「ワークスペース（Workspace）」を作成し、ファイル（<em>Core Keeper</em>のWikiページや仕事の書類など）をドロップします。そして <strong>「Save and Embed（保存して埋め込む）」</strong> をクリックします。</li></ol><h1 id=-ステップ4ナレッジベースを活用する>⚔️ ステップ4：ナレッジベースを活用する<a href=#-ステップ4ナレッジベースを活用する class=hanchor arialabel=Anchor>#</a></h1><p>さあ、あなたのデータとチャットしてみましょう。</p><ul><li><strong>ゲームの質問:</strong> <em>&ldquo;忌まわしき大質量（Abominouse Mass）はどうやって倒せばいい？&rdquo;</em></li><li><strong>仕事の質問:</strong> <em>&ldquo;第3四半期のマーケティング契約の主要な条項を要約して。&rdquo;</em></li></ul><p>Gemma 3 4Bは単に「推測」するだけでなく、あなたのファイルから特定のテキストを見つけ出して解説してくれます。</p><p>独自のナレッジベースを持つ前と後で、AIに質問した時の違いを見てみましょう。</p><ul><li>Before: 文脈がないため、AIが一般的で曖昧、あるいは間違った答えを返している様子。</li></ul><p><img src=/cozy-corner-future/images/building-your-first-rag-system-before.png alt=before></p><ul><li>After: AIがアップロードした文書を引用元として提示し、正確で的確な答えを返している様子。</li></ul><p><img src=/cozy-corner-future/images/building-your-first-rag-system-after.png alt=after></p><h1 id=-自分のデータに応用してみよう>👨🏻‍💻 自分のデータに応用してみよう<a href=#-自分のデータに応用してみよう class=hanchor arialabel=Anchor>#</a></h1><p>これを自分で構築することで、あなたは単なるAIユーザーではなく「設計者（アーキテクト）」になります。ゲームのプレイを最適化する場合でも、仕事のワークフローを改善する場合でも、あなたが知っていることを正確に把握している100%プライベートなオフラインアシスタントを手に入れたことになります。</p><p>ここまでは<em>Core Keeper</em>を例に挙げてきましたが、この「ビルド」は専門的な現場仕事における救世主にもなります。</p><ul><li><strong>フィールド研究者:</strong> <strong>インターネットが全く使えない</strong>大自然の僻地にいると想像してみてください。出発前に、植物図鑑、過去の探検記録、地質図など、ライブラリ全体をAIに学習させておくことができます。</li><li><strong>作家:</strong> 大切な知的財産（IP）をクラウドにアップロードすることなく、下書きの章を読み込ませて世界観の矛盾をチェックできます。</li><li><strong>ホームシェフ:</strong> ごちゃごちゃになったレシピのスクリーンショットフォルダを、検索可能な「デジタル料理本」に変えることができます。</li></ul><blockquote><p><strong>最大のメリット:</strong> <strong>Gemma 3 4B</strong>と<strong>EmbeddingGemma</strong>をローカルで使用するため、システムは100%<strong>オフライン</strong>で動作します。データがPCの外に出ることは一切ないため、衛星回線なしで即座に答えが必要な現場の研究者にとって、完璧なパートナーとなります。</p></blockquote></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://bebechien.github.io/cozy-corner-future/ja/posts/gemini-and-gemma/ class="button inline next">[<span class=button__text>GeminiとGemma、まだどっちがどっちか混乱していませんか？</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/cozy-corner-future/bundle.min.js></script></div></body></html>